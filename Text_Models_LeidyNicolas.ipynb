{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Nicolas Leguizamón, código: 201727960 </span>\n",
    "## <span style=\"color:orange\">Leidy Araque Molina, código: 201727196 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1> <center> <span style=\"color:orange\"> FINAL PROJECT - TEXT MODELS</span></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\AsusPC\\\\Documents\\\\Analytics\\\\2018-Vacacional\\\\DeepLearning\\\\Project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTraining = pd.read_csv(os.path.join(path, 'data', 'dataTraining.csv'), encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv(os.path.join(path, 'data', 'dataTesting.csv'), encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>Message in a Bottle</td>\n",
       "      <td>who meets by fate ,  shall be sealed by fate ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1978</td>\n",
       "      <td>Midnight Express</td>\n",
       "      <td>the true story of billy hayes ,  an american c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>Primal Fear</td>\n",
       "      <td>martin vail left the chicago da ' s office to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1950</td>\n",
       "      <td>Crisis</td>\n",
       "      <td>husband and wife americans dr .  eugene and mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1959</td>\n",
       "      <td>The Tingler</td>\n",
       "      <td>the coroner and scientist dr .  warren chapin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                title  \\\n",
       "1  1999  Message in a Bottle   \n",
       "4  1978     Midnight Express   \n",
       "5  1996          Primal Fear   \n",
       "6  1950               Crisis   \n",
       "7  1959          The Tingler   \n",
       "\n",
       "                                                plot  \n",
       "1  who meets by fate ,  shall be sealed by fate ....  \n",
       "4  the true story of billy hayes ,  an american c...  \n",
       "5  martin vail left the chicago da ' s office to ...  \n",
       "6  husband and wife americans dr .  eugene and mr...  \n",
       "7  the coroner and scientist dr .  warren chapin ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>Message in a Bottle</td>\n",
       "      <td>who meets by fate ,  shall be sealed by fate ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1978</td>\n",
       "      <td>Midnight Express</td>\n",
       "      <td>the true story of billy hayes ,  an american c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>Primal Fear</td>\n",
       "      <td>martin vail left the chicago da ' s office to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1950</td>\n",
       "      <td>Crisis</td>\n",
       "      <td>husband and wife americans dr .  eugene and mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1959</td>\n",
       "      <td>The Tingler</td>\n",
       "      <td>the coroner and scientist dr .  warren chapin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                title  \\\n",
       "1  1999  Message in a Bottle   \n",
       "4  1978     Midnight Express   \n",
       "5  1996          Primal Fear   \n",
       "6  1950               Crisis   \n",
       "7  1959          The Tingler   \n",
       "\n",
       "                                                plot  \n",
       "1  who meets by fate ,  shall be sealed by fate ....  \n",
       "4  the true story of billy hayes ,  an american c...  \n",
       "5  martin vail left the chicago da ' s office to ...  \n",
       "6  husband and wife americans dr .  eugene and mr...  \n",
       "7  the coroner and scientist dr .  warren chapin ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve better results with the present model we are going to use the feature extraction techniques that we learned in the NLP sessions, the idea is to vectorize the feature and extract them to simplify the model and make it more accurate.\n",
    "Character n-grams are a very successful approach to represent text for stylistic purposes since they are able to capture nuances in lexical, syntactical, and structural level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create count vectorizer with ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7895, 1000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=1000)\n",
    "X_dtm = vect.fit_transform(dataTraining['plot'])\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'able to', 'about', 'about the', 'about to', 'accident', 'across', 'act', 'action', 'actually', 'affair', 'after', 'after the', 'again', 'against', 'against the', 'age', 'agent', 'ago', 'alex', 'alive', 'all', 'all of', 'all the', 'alone', 'along', 'along the', 'along with', 'already', 'also', 'although', 'always', 'america', 'american', 'among', 'an', 'an old', 'and', 'and has', 'and he', 'and her', 'and his', 'and is', 'and she', 'and that', 'and the', 'and their', 'and then', 'and they', 'and when']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7895x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 470792 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm, y_genres, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5289x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 316874 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:orange\">1. Machine Learning algorithm </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multi-class multi-label model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=100, max_depth=10, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf = OneVsRestClassifier(BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = OneVsRestClassifier(ExtraTreesClassifier(n_jobs=-1, n_estimators=100, max_depth=10, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_genres = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7569697472601943"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_dtm = vect.transform(dataTesting['plot'])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "y_pred_test_genres = clf.predict_proba(X_test_dtm)\n",
    "\n",
    "pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols).to_csv('pred_text_mach_1.csv', index_label='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:orange\">2. Deep Learning algorithm </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AsusPC\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import History\n",
    "#from livelossplot import PlotLossesKeras\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use the count vectorizer function, we consider these tachniques can be of use in this case since the text is more sucinct and have not so many sinctactical features that can require sophisticated treatment such as in phrases or conversations, also it does not reflect any sentiment that can introduce any Bias or missclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7895, 1500)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(lowercase=False, stop_words='english', max_features=1500) #,ngram_range=(1, 2)\n",
    "X_dtm = vect.fit_transform(dataTraining['plot'])\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24,input_shape=X_dtm.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(24))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 24)                36024     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 36,720\n",
      "Trainable params: 36,672\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7895/7895 [==============================] - 3s 388us/step - loss: 8.7690 - acc: 0.0509\n",
      "Epoch 2/50\n",
      "7895/7895 [==============================] - 1s 132us/step - loss: 8.2945 - acc: 0.1132\n",
      "Epoch 3/50\n",
      "7895/7895 [==============================] - 1s 138us/step - loss: 8.0006 - acc: 0.1738\n",
      "Epoch 4/50\n",
      "7895/7895 [==============================] - 1s 132us/step - loss: 7.7616 - acc: 0.2149\n",
      "Epoch 5/50\n",
      "7895/7895 [==============================] - 1s 142us/step - loss: 7.5208 - acc: 0.2421\n",
      "Epoch 6/50\n",
      "7895/7895 [==============================] - 1s 130us/step - loss: 7.2972 - acc: 0.2441\n",
      "Epoch 7/50\n",
      "7895/7895 [==============================] - 1s 141us/step - loss: 7.1176 - acc: 0.2612\n",
      "Epoch 8/50\n",
      "7895/7895 [==============================] - 1s 137us/step - loss: 6.9540 - acc: 0.2586\n",
      "Epoch 9/50\n",
      "7895/7895 [==============================] - 1s 135us/step - loss: 6.8305 - acc: 0.2509\n",
      "Epoch 10/50\n",
      "7895/7895 [==============================] - 1s 132us/step - loss: 6.7212 - acc: 0.2514\n",
      "Epoch 11/50\n",
      "7895/7895 [==============================] - 1s 130us/step - loss: 6.6281 - acc: 0.2489\n",
      "Epoch 12/50\n",
      "7895/7895 [==============================] - 1s 133us/step - loss: 6.5743 - acc: 0.2521\n",
      "Epoch 13/50\n",
      "7895/7895 [==============================] - 1s 131us/step - loss: 6.5036 - acc: 0.2512\n",
      "Epoch 14/50\n",
      "7895/7895 [==============================] - 1s 126us/step - loss: 6.4292 - acc: 0.2518\n",
      "Epoch 15/50\n",
      "7895/7895 [==============================] - 1s 125us/step - loss: 6.4237 - acc: 0.2509\n",
      "Epoch 16/50\n",
      "7895/7895 [==============================] - 1s 123us/step - loss: 6.3550 - acc: 0.2500\n",
      "Epoch 17/50\n",
      "7895/7895 [==============================] - 1s 134us/step - loss: 6.3239 - acc: 0.2560\n",
      "Epoch 18/50\n",
      "7895/7895 [==============================] - 1s 129us/step - loss: 6.2902 - acc: 0.2531\n",
      "Epoch 19/50\n",
      "7895/7895 [==============================] - 1s 132us/step - loss: 6.2748 - acc: 0.2564\n",
      "Epoch 20/50\n",
      "7895/7895 [==============================] - 1s 130us/step - loss: 6.2328 - acc: 0.2566\n",
      "Epoch 21/50\n",
      "7895/7895 [==============================] - 1s 129us/step - loss: 6.1966 - acc: 0.2562\n",
      "Epoch 22/50\n",
      "7895/7895 [==============================] - 1s 137us/step - loss: 6.1942 - acc: 0.2611\n",
      "Epoch 23/50\n",
      "7895/7895 [==============================] - 1s 139us/step - loss: 6.1564 - acc: 0.2617\n",
      "Epoch 24/50\n",
      "7895/7895 [==============================] - 1s 133us/step - loss: 6.1236 - acc: 0.2693\n",
      "Epoch 25/50\n",
      "7895/7895 [==============================] - 1s 130us/step - loss: 6.1168 - acc: 0.2737\n",
      "Epoch 26/50\n",
      "7895/7895 [==============================] - 1s 129us/step - loss: 6.0830 - acc: 0.2627\n",
      "Epoch 27/50\n",
      "7895/7895 [==============================] - 1s 123us/step - loss: 6.0736 - acc: 0.2779\n",
      "Epoch 28/50\n",
      "7895/7895 [==============================] - 1s 125us/step - loss: 6.0564 - acc: 0.2823\n",
      "Epoch 29/50\n",
      "7895/7895 [==============================] - 1s 121us/step - loss: 6.0061 - acc: 0.2818\n",
      "Epoch 30/50\n",
      "7895/7895 [==============================] - 1s 124us/step - loss: 6.0164 - acc: 0.2823\n",
      "Epoch 31/50\n",
      "7895/7895 [==============================] - 1s 129us/step - loss: 5.9937 - acc: 0.2909\n",
      "Epoch 32/50\n",
      "7895/7895 [==============================] - 1s 123us/step - loss: 5.9785 - acc: 0.2940\n",
      "Epoch 33/50\n",
      "7895/7895 [==============================] - 1s 123us/step - loss: 5.9577 - acc: 0.2870\n",
      "Epoch 34/50\n",
      "7895/7895 [==============================] - 1s 123us/step - loss: 5.9553 - acc: 0.2907\n",
      "Epoch 35/50\n",
      "7895/7895 [==============================] - 1s 123us/step - loss: 5.9245 - acc: 0.2904\n",
      "Epoch 36/50\n",
      "7895/7895 [==============================] - 1s 121us/step - loss: 5.9197 - acc: 0.2927\n",
      "Epoch 37/50\n",
      "7895/7895 [==============================] - 1s 123us/step - loss: 5.9132 - acc: 0.2983\n",
      "Epoch 38/50\n",
      "7895/7895 [==============================] - 1s 122us/step - loss: 5.8866 - acc: 0.2954\n",
      "Epoch 39/50\n",
      "7895/7895 [==============================] - 1s 134us/step - loss: 5.8983 - acc: 0.3064\n",
      "Epoch 40/50\n",
      "7895/7895 [==============================] - 1s 131us/step - loss: 5.8762 - acc: 0.3040\n",
      "Epoch 41/50\n",
      "7895/7895 [==============================] - 1s 127us/step - loss: 5.8726 - acc: 0.3137\n",
      "Epoch 42/50\n",
      "7895/7895 [==============================] - 1s 118us/step - loss: 5.8533 - acc: 0.3073\n",
      "Epoch 43/50\n",
      "7895/7895 [==============================] - 1s 118us/step - loss: 5.8582 - acc: 0.3186\n",
      "Epoch 44/50\n",
      "7895/7895 [==============================] - 1s 116us/step - loss: 5.8581 - acc: 0.3084\n",
      "Epoch 45/50\n",
      "7895/7895 [==============================] - 1s 118us/step - loss: 5.8286 - acc: 0.3101\n",
      "Epoch 46/50\n",
      "7895/7895 [==============================] - 1s 116us/step - loss: 5.8077 - acc: 0.3198\n",
      "Epoch 47/50\n",
      "7895/7895 [==============================] - 1s 119us/step - loss: 5.8302 - acc: 0.3151\n",
      "Epoch 48/50\n",
      "7895/7895 [==============================] - 1s 117us/step - loss: 5.7799 - acc: 0.3263\n",
      "Epoch 49/50\n",
      "7895/7895 [==============================] - 1s 119us/step - loss: 5.7950 - acc: 0.3136\n",
      "Epoch 50/50\n",
      "7895/7895 [==============================] - 1s 118us/step - loss: 5.7816 - acc: 0.3283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28e8146ec50>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_dtm, y_genres, epochs=50, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_dtm = vect.transform(dataTesting['plot'])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "y_pred_test_genres = model.predict_proba(X_test_dtm)\n",
    "\n",
    "pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols).to_csv('pred_text_countVec.csv', index_label='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous model was obtained in <span style=\"color:blue\">KAGGLE 0.8259</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this approach yield to a considerable high score, we can see if we can improve it by means of using anther approach, we know that LSTM can be more suitable for time series such as voice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFID is very useful to extarct features and give the weight to those that resut more frequent, we are going to USE this approach to enhance the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "X_dtm = vect.fit_transform(dataTraining['plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24,input_shape=X_dtm.shape[1:]))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(24))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7895/7895 [==============================] - 11s 1ms/step - loss: 8.0780 - acc: 0.1567\n",
      "Epoch 2/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 7.0518 - acc: 0.2894\n",
      "Epoch 3/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 6.6135 - acc: 0.3326\n",
      "Epoch 4/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 6.3233 - acc: 0.3564\n",
      "Epoch 5/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 6.1115 - acc: 0.3745\n",
      "Epoch 6/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.9386 - acc: 0.3911\n",
      "Epoch 7/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.8255 - acc: 0.3902\n",
      "Epoch 8/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.7347 - acc: 0.3914\n",
      "Epoch 9/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.6637 - acc: 0.4006\n",
      "Epoch 10/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.5713 - acc: 0.4157: 1s - \n",
      "Epoch 11/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.5437 - acc: 0.4043\n",
      "Epoch 12/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.4803 - acc: 0.4144\n",
      "Epoch 13/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.4283 - acc: 0.4187\n",
      "Epoch 14/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.3974 - acc: 0.4180\n",
      "Epoch 15/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.3798 - acc: 0.4193\n",
      "Epoch 16/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.3494 - acc: 0.4180\n",
      "Epoch 17/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.3254 - acc: 0.4241\n",
      "Epoch 18/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.2889 - acc: 0.4208\n",
      "Epoch 19/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.2925 - acc: 0.4206\n",
      "Epoch 20/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.2631 - acc: 0.4276\n",
      "Epoch 21/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.1848 - acc: 0.4276\n",
      "Epoch 22/50\n",
      "7895/7895 [==============================] - 10s 1ms/step - loss: 5.1844 - acc: 0.4286\n",
      "Epoch 23/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.1747 - acc: 0.4236\n",
      "Epoch 24/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.1564 - acc: 0.4232\n",
      "Epoch 25/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.1448 - acc: 0.4328\n",
      "Epoch 26/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.1317 - acc: 0.4315\n",
      "Epoch 27/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.1299 - acc: 0.4358\n",
      "Epoch 28/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0941 - acc: 0.4298\n",
      "Epoch 29/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0724 - acc: 0.4255\n",
      "Epoch 30/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0766 - acc: 0.4371\n",
      "Epoch 31/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0786 - acc: 0.4222\n",
      "Epoch 32/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0674 - acc: 0.4351\n",
      "Epoch 33/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0678 - acc: 0.4361\n",
      "Epoch 34/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0370 - acc: 0.4375\n",
      "Epoch 35/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0409 - acc: 0.4361\n",
      "Epoch 36/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0362 - acc: 0.4318\n",
      "Epoch 37/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 5.0055 - acc: 0.4362\n",
      "Epoch 38/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9980 - acc: 0.4396\n",
      "Epoch 39/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9917 - acc: 0.4385\n",
      "Epoch 40/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9802 - acc: 0.4300\n",
      "Epoch 41/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9984 - acc: 0.4323\n",
      "Epoch 42/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9746 - acc: 0.4352\n",
      "Epoch 43/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9980 - acc: 0.4343\n",
      "Epoch 44/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9713 - acc: 0.4271\n",
      "Epoch 45/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9559 - acc: 0.4336\n",
      "Epoch 46/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9522 - acc: 0.4332\n",
      "Epoch 47/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9575 - acc: 0.4367\n",
      "Epoch 48/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9349 - acc: 0.4303\n",
      "Epoch 49/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9297 - acc: 0.4419\n",
      "Epoch 50/50\n",
      "7895/7895 [==============================] - 9s 1ms/step - loss: 4.9230 - acc: 0.4414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e44fbdb518>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_dtm, y_genres, epochs=50, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_dtm = vect.transform(dataTesting['plot'])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "y_pred_test_genres = model.predict_proba(X_test_dtm)\n",
    "\n",
    "pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols).to_csv('pred_text_TfidV.csv', index_label='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous model was obtained in <span style=\"color:blue\">KAGGLE 0.85603</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "effectively; the approach leads to a better model we are going to keep this in order to average the two models into a single one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using preprocesing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "index_from = 3\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets.imdb import get_word_index\n",
    "\n",
    "vocab = get_word_index()\n",
    "\n",
    "vocab = {k:(v+index_from) for k,v in vocab.items()}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<START>\"] = 1\n",
    "vocab[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34704,\n",
       " 'hanging': 2348,\n",
       " 'nunnery': 52010,\n",
       " 'sonja': 16819,\n",
       " 'spiders': 16118,\n",
       " 'trawling': 52011,\n",
       " 'tsukino': 52009,\n",
       " 'vani': 63954,\n",
       " 'woods': 1411,\n",
       " 'woody': 2292}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:vocab[k] for k in list(vocab.keys())[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = [x[:max_review_length] for x in dataTraining['plot']]\n",
    "X=reviews\n",
    "# Convert characters to int and pad\n",
    "X_voc = [[vocab[x1] for x1 in x if x1 in vocab.keys()] for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = [x[:max_review_length] for x in dataTesting['plot']]\n",
    "X=reviews\n",
    "# Convert characters to int and pad\n",
    "X_test_k = [[vocab[x1] for x1 in x if x1 in vocab.keys()] for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from livelossplot import PlotLossesKeras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_voc_pad = sequence.pad_sequences(X_voc, maxlen=max_review_length)\n",
    "X_test_k_pad = sequence.pad_sequences(X_test_k, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_voc_pad[X_voc_pad>5000]=2\n",
    "X_test_k_pad[X_test_k_pad>5000]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 500, 32)           160032    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 24)                2424      \n",
      "=================================================================\n",
      "Total params: 216,056\n",
      "Trainable params: 215,856\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#embedding es disminucion de dimensionalidad\n",
    "model.add(Embedding(top_words + 1, 32, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(24, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 146s - loss: 7.4838 - acc: 0.1976\n",
      "Epoch 2/20\n",
      " - 147s - loss: 7.4683 - acc: 0.1980\n",
      "Epoch 3/20\n",
      " - 145s - loss: 7.4666 - acc: 0.1977\n",
      "Epoch 4/20\n",
      " - 148s - loss: 7.4601 - acc: 0.1977\n",
      "Epoch 5/20\n",
      " - 144s - loss: 7.4604 - acc: 0.1980\n",
      "Epoch 6/20\n",
      " - 140s - loss: 7.4597 - acc: 0.1978\n",
      "Epoch 7/20\n",
      " - 149s - loss: 7.4607 - acc: 0.1975\n",
      "Epoch 8/20\n",
      " - 145s - loss: 7.4613 - acc: 0.1978\n",
      "Epoch 9/20\n",
      " - 146s - loss: 7.4488 - acc: 0.1978\n",
      "Epoch 10/20\n",
      " - 151s - loss: 7.4554 - acc: 0.1978\n",
      "Epoch 11/20\n",
      " - 183s - loss: 7.4546 - acc: 0.1977\n",
      "Epoch 12/20\n",
      " - 192s - loss: 7.4519 - acc: 0.1978\n",
      "Epoch 13/20\n",
      " - 194s - loss: 7.4554 - acc: 0.1980\n",
      "Epoch 14/20\n",
      " - 197s - loss: 7.4520 - acc: 0.1980\n",
      "Epoch 15/20\n",
      " - 176s - loss: 7.4559 - acc: 0.1978\n",
      "Epoch 16/20\n",
      " - 171s - loss: 7.4574 - acc: 0.1976\n",
      "Epoch 17/20\n",
      " - 142s - loss: 7.4525 - acc: 0.1978\n",
      "Epoch 18/20\n",
      " - 156s - loss: 7.4449 - acc: 0.1978\n",
      "Epoch 19/20\n",
      " - 147s - loss: 7.4552 - acc: 0.1980\n",
      "Epoch 20/20\n",
      " - 145s - loss: 7.4547 - acc: 0.1980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28e99bbb828>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_voc_pad, y_genres, epochs=20, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:orange\">3. Average with Image model and Text model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we did here is to use the model outputs to make the weighted average of the two models (giving a higher consideration of the text model since it has better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_img = pd.read_csv(os.path.join(path, 'pred_genres_img_RF.csv'), encoding='UTF-8', index_col=0)\n",
    "pred_text = pd.read_csv(os.path.join(path, 'pred_text_TfidV.csv'), encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction=0.3*pred_img+0.7*pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "pd.DataFrame(prediction, index=dataTesting.index, columns=cols).to_csv('average.csv', index_label='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
